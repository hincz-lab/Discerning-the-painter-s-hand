{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.feature_extraction.image import extract_patches\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, AveragePooling2D, Dropout, Input\n",
    "from tensorflow.keras import regularizers, optimizers, models, layers, losses, metrics\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report\n",
    "from skimage import img_as_float\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "def load_data(file_path):\n",
    "    data = cv2.imread(file_path,cv2.IMREAD_UNCHANGED)\n",
    "    data = (data/65535)*255\n",
    "    return data\n",
    "\n",
    "\n",
    "# divide each channel of the painting to patches then concatenate channels, also get the list of\n",
    "# corresponding labels (painter id)\n",
    "def get_patches(data, patch_size, painter_id):\n",
    "    pc1 = extract_patches(data[:,:,0], patch_shape = patch_size, extraction_step = patch_size)\n",
    "    pc1 = pc1.reshape(-1, patch_size, patch_size)\n",
    "    pc2 = extract_patches(data[:,:,1], patch_shape = patch_size, extraction_step = patch_size)\n",
    "    pc2 = pc2.reshape(-1, patch_size, patch_size)\n",
    "    pc3 = extract_patches(data[:,:,2], patch_shape = patch_size, extraction_step = patch_size)\n",
    "    pc3 = pc3.reshape(-1, patch_size, patch_size)\n",
    "    pc1_reshaped = pc1.reshape(*pc1.shape,1)\n",
    "    pc2_reshaped = pc2.reshape(*pc2.shape,1)\n",
    "    pc3_reshaped = pc3.reshape(*pc3.shape,1)\n",
    "    patches = np.concatenate((pc1_reshaped,pc2_reshaped,pc3_reshaped),axis=3)\n",
    "\n",
    "    \n",
    "    labels = []\n",
    "    def get_label(painter_id, patch_len):\n",
    "        labels.clear()\n",
    "        labels.append(painter_id * patch_len)\n",
    "        return labels\n",
    "\n",
    "    list_len = np.ones(len(patches))\n",
    "    y_list = get_label(painter_id, list_len)\n",
    "    y_list = np.reshape(y_list,(len(patches),1)) \n",
    "                        \n",
    "    return patches, y_list  # use this when shuffle=False\n",
    "\n",
    "\n",
    "# preprocess each patches to prepare for transfer learning by subtracting the mean [103.939, 116.779, 123.68]\n",
    "def preprocess_patches(patch_list):\n",
    "    patches = preprocess_input(patch_list)\n",
    "    return patches\n",
    "\n",
    "# resize patches to 224*224\n",
    "def resize_patches(patch_list):   \n",
    "    resize_patches = [None]*len(patch_list)\n",
    "    for i in range(len(patch_list)):\n",
    "        resize_patches[i] = cv2.resize(patch_list[i],(224, 224))\n",
    "    new_list = np.asarray(resize_patches, dtype=np.float64)\n",
    "    return new_list\n",
    "    \n",
    "# put all the previous step together\n",
    "def process_pipeline(file_path, patch_size, painter_id):\n",
    "    data = load_data(file_path)\n",
    "    patch_list, labels = get_patches(data, patch_size, painter_id)\n",
    "    preprocessed_patches = preprocess_patches(patch_list)\n",
    "    resized_patches = resize_patches(preprocessed_patches)\n",
    "    return resized_patches, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p1a_imf_arr = ['emd_painting_1_imf_1.png','emd_painting_1_imf_2.png','emd_painting_1_imf_3.png','emd_painting_1_imf_4.png','emd_painting_1_imf_5.png']\n",
    "p1b_imf_arr = ['emd_painting_2_imf_1.png','emd_painting_2_imf_2.png','emd_painting_2_imf_3.png','emd_painting_2_imf_4.png','emd_painting_2_imf_5.png']\n",
    "p1c_imf_arr = ['emd_painting_3_imf_1.png','emd_painting_3_imf_2.png','emd_painting_3_imf_3.png','emd_painting_3_imf_4.png','emd_painting_3_imf_5.png']\n",
    "p2a_imf_arr = ['emd_painting_4_imf_1.png','emd_painting_4_imf_2.png','emd_painting_4_imf_3.png','emd_painting_4_imf_4.png','emd_painting_4_imf_5.png']\n",
    "p2b_imf_arr = ['emd_painting_5_imf_1.png','emd_painting_5_imf_2.png','emd_painting_5_imf_3.png','emd_painting_5_imf_4.png','emd_painting_5_imf_5.png']\n",
    "p2c_imf_arr = ['emd_painting_6_imf_1.png','emd_painting_6_imf_2.png','emd_painting_6_imf_3.png','emd_painting_6_imf_4.png','emd_painting_6_imf_5.png']\n",
    "p3a_imf_arr = ['emd_painting_7_imf_1.png','emd_painting_7_imf_2.png','emd_painting_7_imf_3.png','emd_painting_7_imf_4.png','emd_painting_7_imf_5.png']\n",
    "p3b_imf_arr = ['emd_painting_8_imf_1.png','emd_painting_8_imf_2.png','emd_painting_8_imf_3.png','emd_painting_8_imf_4.png','emd_painting_8_imf_5.png']\n",
    "p3c_imf_arr = ['emd_painting_9_imf_1.png','emd_painting_9_imf_2.png','emd_painting_9_imf_3.png','emd_painting_9_imf_4.png','emd_painting_9_imf_5.png']\n",
    "p4a_imf_arr = ['emd_painting_10_imf_1.png','emd_painting_10_imf_2.png','emd_painting_10_imf_3.png','emd_painting_10_imf_4.png','emd_painting_10_imf_5.png']\n",
    "p4b_imf_arr = ['emd_painting_11_imf_1.png','emd_painting_11_imf_2.png','emd_painting_11_imf_3.png','emd_painting_11_imf_4.png','emd_painting_11_imf_5.png']\n",
    "p4c_imf_arr = ['emd_painting_12_imf_1.png','emd_painting_12_imf_2.png','emd_painting_12_imf_3.png','emd_painting_12_imf_4.png','emd_painting_12_imf_5.png']\n",
    "\n",
    "\n",
    "for imf_i in [3]: #range(len(p1a_imf_arr)): # loop through imfs, imf_i = 0-4 representing imf1-5\n",
    "    \n",
    "    psizes = [200] #patch size in pixels\n",
    "\n",
    "    for patch_size in psizes:\n",
    "        print('PATCH SIZE: '+repr(patch_size))\n",
    "        # get a list of patches (x) with corresponding painter if (y) for all 12 paintings\n",
    "        p1a_x, p1a_y = process_pipeline(p1a_imf_arr[imf_i], patch_size, 0)\n",
    "        p1b_x, p1b_y = process_pipeline(p1b_imf_arr[imf_i], patch_size, 0)\n",
    "        p1c_x, p1c_y = process_pipeline(p1c_imf_arr[imf_i], patch_size, 0)\n",
    "        p2a_x, p2a_y = process_pipeline(p2a_imf_arr[imf_i], patch_size, 1)\n",
    "        p2b_x, p2b_y = process_pipeline(p2b_imf_arr[imf_i], patch_size, 1)\n",
    "        p2c_x, p2c_y = process_pipeline(p2c_imf_arr[imf_i], patch_size, 1)\n",
    "        p3a_x, p3a_y = process_pipeline(p3a_imf_arr[imf_i], patch_size, 2)\n",
    "        p3b_x, p3b_y = process_pipeline(p3b_imf_arr[imf_i], patch_size, 2)\n",
    "        p3c_x, p3c_y = process_pipeline(p3c_imf_arr[imf_i], patch_size, 2)\n",
    "        p4a_x, p4a_y = process_pipeline(p4a_imf_arr[imf_i], patch_size, 3)\n",
    "        p4c_x, p4c_y = process_pipeline(p4c_imf_arr[imf_i], patch_size, 3)\n",
    "        p4_b = cv2.imread(p4b_imf_arr[imf_i],cv2.IMREAD_UNCHANGED)\n",
    "        p4_b = (p4_b/65535)*255\n",
    "        p4_b = cv2.rotate(p4_b, cv2.ROTATE_180) # this painting is upside down so needs to be rotated\n",
    "        p4b_x, p4b_y = get_patches(p4_b, patch_size, 3)\n",
    "        p4b_x = resize_patches(preprocess_patches(p4b_x))\n",
    "\n",
    "        x_train_val = np.concatenate((p1a_x, p1c_x, \n",
    "                                      p2a_x, p2c_x, \n",
    "                                      p3a_x, p3c_x, \n",
    "                                      p4a_x, p4c_x))\n",
    "        y_train_val = np.concatenate((p1a_y, p1c_y, \n",
    "                                      p2a_y, p2c_y, \n",
    "                                      p3a_y, p3c_y, \n",
    "                                      p4a_y, p4c_y))\n",
    "        del(p1a_x,p1a_y,p2a_x,p2a_y,p3a_x,p3a_y,p4a_x,p4a_y)\n",
    "        del(p1c_x,p1c_y,p2c_x,p2c_y,p3c_x,p3c_y,p4c_x,p4c_y)\n",
    "\n",
    "\n",
    "        foldnum=20\n",
    "\n",
    "        for fold in range(0, foldnum):\n",
    "            print('PATCH SIZE: '+repr(patch_size))\n",
    "\n",
    "            x_train,x_val,y_train,y_val = train_test_split(x_train_val, y_train_val, test_size=0.1)\n",
    "\n",
    "            x_test = np.concatenate((p1b_x, p2b_x, p3b_x, p4b_x))\n",
    "\n",
    "            # one-hot encode y\n",
    "            y_train = to_categorical(y_train, num_classes=None)\n",
    "            y_val = to_categorical(y_val, num_classes=None)\n",
    "\n",
    "            y_test = np.concatenate((p1b_y, p2b_y, p3b_y, p4b_y))\n",
    "            y_test = to_categorical(y_test, num_classes=None)\n",
    "\n",
    "            # get index for all the patches in the testing painting\n",
    "            test_idx = np.arange(len(p1b_x)).reshape(len(p1b_x),1)\n",
    "\n",
    "\n",
    "            #################\n",
    "            #if run into \"value error\", copy the rest of the code to a new cell and rerun it\n",
    "            # \"value error\" happens when the frozen network performs better than the network with layers unlocked\n",
    "            #################\n",
    "\n",
    "            baseModel = VGG16(weights=\"imagenet\", include_top=False,input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "            model = models.Sequential()\n",
    "            model.add(baseModel)\n",
    "            model.add(layers.AveragePooling2D(pool_size=(3, 3)))\n",
    "            model.add(layers.Flatten())\n",
    "            model.add(layers.Dropout(0.25))\n",
    "            model.add(layers.Dense(64, activation='relu',kernel_regularizer=regularizers.l2(0.001)))\n",
    "            model.add(layers.Dropout(0.25))\n",
    "            model.add(layers.Dense(4, activation=\"softmax\"))\n",
    "\n",
    "            for layer in baseModel.layers[:]:\n",
    "                layer.trainable = False\n",
    "\n",
    "\n",
    "            model.compile(optimizer=optimizers.Adam(lr = 0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            filepath= \"weights.best.hdf5\"\n",
    "            checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "            callbacks_list = [checkpoint]\n",
    "            \n",
    "            try:\n",
    "                # train ONLY top layers \n",
    "                history = model.fit(x_train, y_train, epochs=25, batch_size=32, validation_data=(x_val,y_val),shuffle=True,callbacks=callbacks_list, verbose=2)\n",
    "\n",
    "                #load the best top model\n",
    "                model.load_weights(filepath)\n",
    "\n",
    "                # Make last two blocks of the baseModel trainable:\n",
    "                for layer in baseModel.layers[:11]:\n",
    "                    layer.trainable = False\n",
    "                for layer in baseModel.layers[11:]:\n",
    "                    layer.trainable = True\n",
    "\n",
    "\n",
    "                # Compile frozen baseModel + unfrozen top block + my top layer\n",
    "                model.compile(optimizer=optimizers.Adam(lr = 0.0001),\n",
    "                              loss='categorical_crossentropy',\n",
    "                              metrics=['accuracy'])\n",
    "\n",
    "                #train with a slower learning rate\n",
    "                history = model.fit(x_train, y_train, epochs=25, batch_size=32, validation_data=(x_val,y_val),shuffle=True,callbacks=callbacks_list,verbose=2)\n",
    "\n",
    "                try:\n",
    "                    model.load_weights(filepath)\n",
    "\n",
    "                    model.compile(optimizer=optimizers.Adam(lr = 0.0001), loss='categorical_crossentropy', \n",
    "                              metrics=['accuracy'])\n",
    "\n",
    "                    '''   \n",
    "                    if os.path.exists(filepath):\n",
    "                        os.remove(\"demofile.txt\")\n",
    "                    else:\n",
    "                        print(\"The file does not exist\")\n",
    "\n",
    "\n",
    "                    path_del = (filepath)     \n",
    "                    try:\n",
    "                        os.rmdir(filepath)\n",
    "                    except OSError:\n",
    "                        print (\"Deletion of the file %s failed\" % filepath)\n",
    "                    else:\n",
    "                        print (\"Successfully deleted the file %s\" % filepath)\n",
    "                    '''\n",
    "\n",
    "                    y_pred = model.predict(x_test)\n",
    "                    ypred = np.argmax(y_pred, axis=1)\n",
    "                    ytest = np.argmax(y_test, axis=1)\n",
    "                    \n",
    "                    # resulting coufusion matrix\n",
    "                    cm = confusion_matrix(ytest, ypred)\n",
    "                    cm_flatten = cm.flatten()\n",
    "                    ps_cm = np.insert(cm_flatten,0,patch_size)\n",
    "                    ps_cm = ps_cm.reshape(1,17)\n",
    "\n",
    "                    test_accuracy = (np.trace(cm))/len(ytest)\n",
    "                    print('RESULT: '+repr(patch_size)+', '+repr(test_accuracy)+'\\n')\n",
    "\n",
    "                    p1predict = model.predict(p1b_x)\n",
    "                    p2predict = model.predict(p2b_x)\n",
    "                    p3predict = model.predict(p3b_x)\n",
    "                    p4predict = model.predict(p4b_x)\n",
    "\n",
    "                    p1_predict = np.concatenate([test_idx,p1predict], axis=1)\n",
    "                    p2_predict = np.concatenate([test_idx,p2predict], axis=1)\n",
    "                    p3_predict = np.concatenate([test_idx,p3predict], axis=1)\n",
    "                    p4_predict = np.concatenate([test_idx,p4predict], axis=1)\n",
    "\n",
    "                    report = classification_report(ytest, ypred,output_dict=True)\n",
    "                    p1_report = np.asarray([report['0']['f1-score']])\n",
    "                    p1_report = np.insert(p1_report,0,patch_size)\n",
    "                    p1_report = p1_report.reshape(1,2)\n",
    "\n",
    "                    p2_report = np.asarray([report['1']['f1-score']])\n",
    "                    p2_report = np.insert(p2_report,0,patch_size)\n",
    "                    p2_report = p2_report.reshape(1,2)\n",
    "\n",
    "                    p3_report = np.asarray([report['2']['f1-score']])\n",
    "                    p3_report = np.insert(p3_report,0,patch_size)\n",
    "                    p3_report = p3_report.reshape(1,2)\n",
    "\n",
    "                    p4_report = np.asarray([report['3']['f1-score']])\n",
    "                    p4_report = np.insert(p4_report,0,patch_size)\n",
    "                    p4_report = p4_report.reshape(1,2)\n",
    "\n",
    "                    overall = np.asarray([report['accuracy'],report['macro avg']['f1-score'],report['weighted avg']['f1-score'] ])\n",
    "                    overall = np.insert(overall,0,patch_size)\n",
    "                    overall = overall.reshape(1,4)\n",
    "\n",
    "\n",
    "                    with open('p1_report_ps'+repr(patch_size)+'_emd_mike_individual_imf'+repr(imf_i+1)+'.csv','a') as f:\n",
    "                        np.savetxt(f, p1_report, fmt='%s')\n",
    "                    with open('p2_report_ps'+repr(patch_size)+'_emd_mike_individual_imf'+repr(imf_i+1)+'.csv','a') as f:\n",
    "                        np.savetxt(f, p2_report, fmt='%s')\n",
    "                    with open('p3_report_ps'+repr(patch_size)+'_emd_mike_individual_imf'+repr(imf_i+1)+'.csv','a') as f:\n",
    "                        np.savetxt(f, p3_report, fmt='%s')\n",
    "                    with open('p4_report_ps'+repr(patch_size)+'_emd_mike_individual_imf'+repr(imf_i+1)+'.csv','a') as f:\n",
    "                        np.savetxt(f, p4_report, fmt='%s')\n",
    "                    with open('overall_report_ps'+repr(patch_size)+'_emd_mike_individual_imf'+repr(imf_i+1)+'.csv','a') as f:\n",
    "                        np.savetxt(f, overall, fmt='%s')\n",
    "\n",
    "\n",
    "                    with open('heapmap_p1_ps'+repr(patch_size)+'_emd_mike_individual_imf'+repr(imf_i+1)+'.csv','a') as f:\n",
    "                        np.savetxt(f, p1_predict, fmt='%s')\n",
    "                    with open('heapmap_p2_ps'+repr(patch_size)+'_emd_mike_individual_imf'+repr(imf_i+1)+'.csv','a') as f:\n",
    "                        np.savetxt(f, p2_predict, fmt='%s')\n",
    "                    with open('heapmap_p3_ps'+repr(patch_size)+'_emd_mike_individual_imf'+repr(imf_i+1)+'.csv','a') as f:\n",
    "                        np.savetxt(f, p3_predict, fmt='%s')\n",
    "                    with open('heapmap_p4_ps'+repr(patch_size)+'_emd_mike_individual_imf'+repr(imf_i+1)+'.csv','a') as f:\n",
    "                        np.savetxt(f, p4_predict, fmt='%s')\n",
    "                    with open('cm_height_vgg16_ps'+repr(patch_size)+'_emd_mike_individual_imf'+repr(imf_i+1)+'.csv','a') as f:\n",
    "                        np.savetxt(f, ps_cm, fmt='%s')\n",
    "\n",
    "                    with open(\"accuracy_vgg16_ps\"+repr(patch_size)+'_emd_mike_individual_imf'+repr(imf_i+1)+'.csv', \"a\") as myfile:\n",
    "                        myfile.write(repr(patch_size)+','+repr(test_accuracy)+'\\n')\n",
    "\n",
    "                    del(p1predict,p2predict,p3predict,p4predict,p1_predict,p2_predict,p3_predict,p4_predict,model,ps_cm,cm,history)\n",
    "\n",
    "                except:\n",
    "                    pass   \n",
    "            except:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_accuracy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
