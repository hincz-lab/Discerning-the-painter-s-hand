{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.feature_extraction.image import extract_patches\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Flatten, AveragePooling2D, Dropout, Input\n",
    "from keras import regularizers, optimizers, models, layers, losses, metrics\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report\n",
    "from skimage import img_as_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    data = cv2.imread(file_path,cv2.IMREAD_UNCHANGED)\n",
    "    data = img_as_float(data)*255\n",
    "    return data\n",
    "\n",
    "\n",
    "# divide each channel of the painting to patches then concatenate channels, also get the list of\n",
    "# corresponding labels (painter id)\n",
    "def get_patches(data, patch_size, painter_id):\n",
    "    pc1 = extract_patches(data[:,:,0], patch_shape = patch_size, extraction_step = patch_size)\n",
    "    pc1 = pc1.reshape(-1, patch_size, patch_size)\n",
    "    pc2 = extract_patches(data[:,:,1], patch_shape = patch_size, extraction_step = patch_size)\n",
    "    pc2 = pc2.reshape(-1, patch_size, patch_size)\n",
    "    pc3 = extract_patches(data[:,:,2], patch_shape = patch_size, extraction_step = patch_size)\n",
    "    pc3 = pc3.reshape(-1, patch_size, patch_size)\n",
    "    pc1_reshaped = pc1.reshape(*pc1.shape,1)\n",
    "    pc2_reshaped = pc2.reshape(*pc2.shape,1)\n",
    "    pc3_reshaped = pc3.reshape(*pc3.shape,1)\n",
    "    patches = np.concatenate((pc3_reshaped,pc3_reshaped,pc2_reshaped),axis=3)\n",
    "    \n",
    "    labels = []\n",
    "    def get_label(painter_id, patch_len):\n",
    "        labels.clear()\n",
    "        labels.append(painter_id * patch_len)\n",
    "        return labels\n",
    "\n",
    "    list_len = np.ones(len(patches))\n",
    "    y_list = get_label(painter_id, list_len)\n",
    "    y_list = np.reshape(y_list,(len(patches),1)) \n",
    "                        \n",
    "    return patches, y_list  # use this when shuffle=False\n",
    "\n",
    "\n",
    "# preprocess each patches to prepare for transfer learning by subtracting the mean [103.939, 116.779, 123.68]\n",
    "def preprocess_patches(patch_list):\n",
    "    patches = preprocess_input(patch_list)\n",
    "    return patches\n",
    "\n",
    "# resize patches to 224*224\n",
    "def resize_patches(patch_list):   \n",
    "    resize_patches = [None]*len(patch_list)\n",
    "    for i in range(len(patch_list)):\n",
    "        resize_patches[i] = cv2.resize(patch_list[i],(224, 224))\n",
    "    new_list = np.asarray(resize_patches, dtype=np.float64)\n",
    "    return new_list\n",
    "    \n",
    "\n",
    "def process_pipeline(file_path, patch_size, painter_id):\n",
    "    data = load_data(file_path)\n",
    "    patch_list, labels = get_patches(data, patch_size, painter_id)\n",
    "    preprocessed_patches = preprocess_patches(patch_list)\n",
    "    resized_patches = resize_patches(preprocessed_patches)\n",
    "    return resized_patches, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#psizes = [200, 224]\n",
    "psizes = [100]#140,160,180,200,224,250,300,75]\n",
    "#psizes = [800]\n",
    "#psizes=[100,140,160,180,200,224,250,300,350,400,500,600,700,800]#, 900, 1000, 1100, 1200]\n",
    "\n",
    "\n",
    "for patch_size in psizes:\n",
    "    foldnum=100\n",
    "    \n",
    "    for fold in range(0, foldnum):\n",
    "        print('PATCH SIZE: '+repr(patch_size))\n",
    "\n",
    "\n",
    "        p1a_x, p1a_y = process_pipeline('/home/ml/FINAL_CODE_DATA/height_data/fgp1.png', patch_size, 0)\n",
    "        p1b_x, p1b_y = process_pipeline('/home/ml/FINAL_CODE_DATA/height_data/fgp2.png', patch_size, 0)\n",
    "        p1c_x, p1c_y = process_pipeline('/home/ml/FINAL_CODE_DATA/height_data/fgp3.png', patch_size, 0)\n",
    "        p2a_x, p2a_y = process_pipeline('/home/ml/FINAL_CODE_DATA/height_data/fgp4.png', patch_size, 1)\n",
    "        p2b_x, p2b_y = process_pipeline('/home/ml/FINAL_CODE_DATA/height_data/fgp5.png', patch_size, 1)\n",
    "        p2c_x, p2c_y = process_pipeline('/home/ml/FINAL_CODE_DATA/height_data/fgp6.png', patch_size, 1)\n",
    "        p3a_x, p3a_y = process_pipeline('/home/ml/FINAL_CODE_DATA/height_data/fgp7.png', patch_size, 2)\n",
    "        p3b_x, p3b_y = process_pipeline('/home/ml/FINAL_CODE_DATA/height_data/fgp8.png', patch_size, 2)\n",
    "        p3c_x, p3c_y = process_pipeline('/home/ml/FINAL_CODE_DATA/height_data/fgp9.png', patch_size, 2)\n",
    "        p4a_x, p4a_y = process_pipeline('/home/ml/FINAL_CODE_DATA/height_data/fgp10.png', patch_size, 3)\n",
    "        p4c_x, p4c_y = process_pipeline('/home/ml/FINAL_CODE_DATA/height_data/fgp12.png', patch_size, 3)\n",
    "\n",
    "        p4_b = cv2.imread('/home/ml/FINAL_CODE_DATA/height_data/fgp11.png',cv2.IMREAD_UNCHANGED)  \n",
    "        p4_b = img_as_float(p4_b)*255\n",
    "        p4_b = cv2.rotate(p4_b, cv2.ROTATE_180)\n",
    "        p4b_x, p4b_y = get_patches(p4_b, patch_size, 3)\n",
    "        p4b_x = resize_patches(preprocess_patches(p4b_x))\n",
    "        \n",
    "        del p4_b\n",
    "        \n",
    "        masks = np.loadtxt('/home/ml/Gundeep/main/mask_info_with_borders.csv',delimiter=\",\")\n",
    "        \n",
    "        bckg1a_x = p1a_x[masks[0,:]==1,:,:,:]\n",
    "        bckg1a_y = p1a_y[masks[0,:]==1,:]\n",
    "        \n",
    "        bckg1b_x = p1a_x[masks[1,:]==1,:,:,:]\n",
    "        bckg1b_y = p1a_y[masks[1,:]==1,:]\n",
    "        \n",
    "        bckg1c_x = p1c_x[masks[2,:]==1,:,:,:]\n",
    "        bckg1c_y = p1c_y[masks[2,:]==1,:]\n",
    "        \n",
    "        bckg2a_x = p2a_x[masks[3,:]==1,:,:,:]\n",
    "        bckg2a_y = p2a_y[masks[3,:]==1,:]        \n",
    "        \n",
    "        bckg2b_x = p2b_x[masks[4,:]==1,:,:,:]\n",
    "        bckg2b_y = p2b_y[masks[4,:]==1,:]        \n",
    "        \n",
    "        bckg2c_x = p2c_x[masks[5,:]==1,:,:,:]\n",
    "        bckg2c_y = p2c_y[masks[5,:]==1,:]        \n",
    "                \n",
    "        bckg3a_x = p3a_x[masks[6,:]==1,:,:,:]\n",
    "        bckg3a_y = p3a_y[masks[6,:]==1,:]        \n",
    "        \n",
    "        bckg3b_x = p3b_x[masks[7,:]==1,:,:,:]\n",
    "        bckg3b_y = p3b_y[masks[7,:]==1,:]        \n",
    "        \n",
    "        bckg3c_x = p3c_x[masks[8,:]==1,:,:,:]\n",
    "        bckg3c_y = p3c_y[masks[8,:]==1,:]                \n",
    "        \n",
    "        bckg4a_x = p4a_x[masks[9,:]==1,:,:,:]\n",
    "        bckg4a_y = p4a_y[masks[9,:]==1,:]        \n",
    "        \n",
    "        bckg4b_x = p4b_x[masks[10,:]==1,:,:,:]\n",
    "        bckg4b_y = p4b_y[masks[10,:]==1,:]        \n",
    "        \n",
    "        bckg4c_x = p4c_x[masks[11,:]==1,:,:,:]\n",
    "        bckg4c_y = p4c_y[masks[11,:]==1,:]                \n",
    "        \n",
    "        \n",
    "        frgrd1a_x = p1a_x[masks[0,:]==0,:,:,:]\n",
    "        frgrd1a_y = p1a_y[masks[0,:]==0,:]\n",
    "        \n",
    "        frgrd1b_x = p1a_x[masks[1,:]==0,:,:,:]\n",
    "        frgrd1b_y = p1a_y[masks[1,:]==0,:]\n",
    "        \n",
    "        frgrd1c_x = p1c_x[masks[2,:]==0,:,:,:]\n",
    "        frgrd1c_y = p1c_y[masks[2,:]==0,:]\n",
    "        \n",
    "        frgrd2a_x = p2a_x[masks[3,:]==0,:,:,:]\n",
    "        frgrd2a_y = p2a_y[masks[3,:]==0,:]        \n",
    "        \n",
    "        frgrd2b_x = p2b_x[masks[4,:]==0,:,:,:]\n",
    "        frgrd2b_y = p2b_y[masks[4,:]==0,:]        \n",
    "        \n",
    "        frgrd2c_x = p2c_x[masks[5,:]==0,:,:,:]\n",
    "        frgrd2c_y = p2c_y[masks[5,:]==0,:]        \n",
    "                \n",
    "        frgrd3a_x = p3a_x[masks[6,:]==0,:,:,:]\n",
    "        frgrd3a_y = p3a_y[masks[6,:]==0,:]        \n",
    "        \n",
    "        frgrd3b_x = p3b_x[masks[7,:]==0,:,:,:]\n",
    "        frgrd3b_y = p3b_y[masks[7,:]==0,:]        \n",
    "        \n",
    "        frgrd3c_x = p3c_x[masks[8,:]==0,:,:,:]\n",
    "        frgrd3c_y = p3c_y[masks[8,:]==0,:]                \n",
    "        \n",
    "        frgrd4a_x = p4a_x[masks[9,:]==0,:,:,:]\n",
    "        frgrd4a_y = p4a_y[masks[9,:]==0,:]        \n",
    "        \n",
    "        frgrd4b_x = p4b_x[masks[10,:]==0,:,:,:]\n",
    "        frgrd4b_y = p4b_y[masks[10,:]==0,:]        \n",
    "        \n",
    "        frgrd4c_x = p4c_x[masks[11,:]==0,:,:,:]\n",
    "        frgrd4c_y = p4c_y[masks[11,:]==0,:]                \n",
    "        \n",
    "        del (p1a_x, p2a_x, p3a_x, p4a_x, p1b_x, p2b_x, p3b_x, p4b_x, p1c_x, p2c_x, p3c_x, p4c_x)\n",
    "        del (p1a_y, p2a_y, p3a_y, p4a_y, p1b_y, p2b_y, p3b_y, p4b_y, p1c_y, p2c_y, p3c_y, p4c_y)\n",
    "        \n",
    "        x_train_val = np.concatenate((bckg1a_x, bckg1b_x, bckg1c_x,\n",
    "                                      bckg2a_x, bckg2b_x, bckg2c_x,\n",
    "                                      bckg3a_x, bckg3b_x, bckg3c_x,\n",
    "                                      bckg4a_x, bckg4b_x, bckg4c_x))\n",
    "        \n",
    "        y_train_val = np.concatenate((bckg1a_y, bckg1b_y, bckg1c_y,\n",
    "                                      bckg2a_y, bckg2b_y, bckg2c_y,\n",
    "                                      bckg3a_y, bckg3b_y, bckg3c_y,\n",
    "                                      bckg4a_y, bckg4b_y, bckg4c_y))\n",
    "        '''\n",
    "        x_train_val = np.concatenate((frgrd1a_x, frgrd1b_x, frgrd1c_x,\n",
    "                                      frgrd2a_x, frgrd2b_x, frgrd2c_x,\n",
    "                                      frgrd3a_x, frgrd3b_x, frgrd3c_x,\n",
    "                                      frgrd4a_x, frgrd4b_x, frgrd4c_x))\n",
    "        \n",
    "        y_train_val = np.concatenate((frgrd1a_y, frgrd1b_y, frgrd1c_y,\n",
    "                                      frgrd2a_y, frgrd2b_y, frgrd2c_y,\n",
    "                                      frgrd3a_y, frgrd3b_y, frgrd3c_y,\n",
    "                                      frgrd4a_y, frgrd4b_y, frgrd4c_y))\n",
    "        \n",
    "        \n",
    "        \n",
    "        del (frgrd1a_x, frgrd1b_x, frgrd1c_x,\n",
    "             frgrd2a_x, frgrd2b_x, frgrd2c_x,\n",
    "             frgrd3a_x, frgrd3b_x, frgrd3c_x,\n",
    "             frgrd4a_x, frgrd4b_x, frgrd4c_x)\n",
    "        \n",
    "        del (frgrd1a_y, frgrd1b_y, frgrd1c_y,\n",
    "             frgrd2a_y, frgrd2b_y, frgrd2c_y,\n",
    "             frgrd3a_y, frgrd3b_y, frgrd3c_y,\n",
    "             frgrd4a_y, frgrd4b_y, frgrd4c_y)\n",
    "        \n",
    "        '''\n",
    "        del (bckg1a_x, bckg1b_x, bckg1c_x,\n",
    "             bckg2a_x, bckg2b_x, bckg2c_x, \n",
    "             bckg3a_x, bckg3b_x, bckg3c_x,\n",
    "             bckg4a_x, bckg4b_x, bckg4c_x)\n",
    "        \n",
    "        del (bckg1a_y, bckg1b_y, bckg1c_y,\n",
    "             bckg2a_y, bckg2b_y, bckg2c_y, \n",
    "             bckg3a_y, bckg3b_y, bckg3c_y,\n",
    "             bckg4a_y, bckg4b_y, bckg4c_y)\n",
    "        \n",
    "        \n",
    "        x_train,x_val,y_train,y_val = train_test_split(x_train_val, y_train_val, test_size=0.1)\n",
    "        \n",
    "        del (x_train_val,y_train_val)\n",
    "        \n",
    "        \n",
    "        test1x = np.concatenate((frgrd1a_x, frgrd1b_x, frgrd1c_x))\n",
    "        test2x = np.concatenate((frgrd2a_x, frgrd2b_x, frgrd2c_x))\n",
    "        test3x = np.concatenate((frgrd3a_x, frgrd3b_x, frgrd3c_x))\n",
    "        test4x = np.concatenate((frgrd4a_x, frgrd4b_x, frgrd4c_x))\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        test1x = np.concatenate((bckg1a_x, bckg1b_x, bckg1c_x))\n",
    "        test2x = np.concatenate((bckg2a_x, bckg2b_x, bckg2c_x))\n",
    "        test3x = np.concatenate((bckg3a_x, bckg3b_x, bckg3c_x))\n",
    "        test4x = np.concatenate((bckg4a_x, bckg4b_x, bckg4c_x))\n",
    "        '''\n",
    "        \n",
    "        test_idx1 = np.arange(len(test1x)).reshape(len(test1x),1)\n",
    "        test_idx2 = np.arange(len(test2x)).reshape(len(test2x),1)\n",
    "        test_idx3 = np.arange(len(test3x)).reshape(len(test3x),1)\n",
    "        test_idx4 = np.arange(len(test4x)).reshape(len(test4x),1)\n",
    "        \n",
    "        #x_test = np.concatenate((frgrd1a_x, frgrd1b_x, frgrd1c_x,\n",
    "        #                         frgrd2a_x, frgrd2b_x, frgrd2c_x,\n",
    "        #                         frgrd3a_x, frgrd3b_x, frgrd3c_x,\n",
    "        #                         frgrd4a_x, frgrd4b_x, frgrd4c_x))\n",
    "        \n",
    "        '''\n",
    "        y_test = np.concatenate((bckg1a_y, bckg1b_y, bckg1c_y,\n",
    "                                 bckg2a_y, bckg2b_y, bckg2c_y,\n",
    "                                 bckg3a_y, bckg3b_y, bckg3c_y,\n",
    "                                 bckg4a_y, bckg4b_y, bckg4c_y))\n",
    "        '''\n",
    "\n",
    "        y_test = np.concatenate((frgrd1a_y, frgrd1b_y, frgrd1c_y,\n",
    "                                 frgrd2a_y, frgrd2b_y, frgrd2c_y,\n",
    "                                 frgrd3a_y, frgrd3b_y, frgrd3c_y,\n",
    "                                 frgrd4a_y, frgrd4b_y, frgrd4c_y))\n",
    "        '''\n",
    "        \n",
    "        del (bckg1a_x, bckg1b_x, bckg1c_x,\n",
    "             bckg2a_x, bckg2b_x, bckg2c_x, \n",
    "             bckg3a_x, bckg3b_x, bckg3c_x,\n",
    "             bckg4a_x, bckg4b_x, bckg4c_x)\n",
    "        \n",
    "        del (bckg1a_y, bckg1b_y, bckg1c_y,\n",
    "             bckg2a_y, bckg2b_y, bckg2c_y, \n",
    "             bckg3a_y, bckg3b_y, bckg3c_y,\n",
    "             bckg4a_y, bckg4b_y, bckg4c_y)\n",
    "        \n",
    "        '''\n",
    "        del (frgrd1a_x, frgrd1b_x, frgrd1c_x,\n",
    "             frgrd2a_x, frgrd2b_x, frgrd2c_x,\n",
    "             frgrd3a_x, frgrd3b_x, frgrd3c_x,\n",
    "             frgrd4a_x, frgrd4b_x, frgrd4c_x)\n",
    "        \n",
    "        del (frgrd1a_y, frgrd1b_y, frgrd1c_y,\n",
    "             frgrd2a_y, frgrd2b_y, frgrd2c_y,\n",
    "             frgrd3a_y, frgrd3b_y, frgrd3c_y,\n",
    "             frgrd4a_y, frgrd4b_y, frgrd4c_y)\n",
    "\n",
    "        # one-hot encode y\n",
    "        y_train = to_categorical(y_train, num_classes=None)\n",
    "        y_val = to_categorical(y_val, num_classes=None)\n",
    "        y_test = to_categorical(y_test, num_classes=None)\n",
    "\n",
    "        baseModel = VGG16(weights=\"imagenet\", include_top=False,input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "        model = models.Sequential()\n",
    "        model.add(baseModel)\n",
    "        model.add(layers.AveragePooling2D(pool_size=(3, 3)))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dropout(0.25))\n",
    "        model.add(layers.Dense(64, activation='relu',kernel_regularizer=regularizers.l2(0.001)))\n",
    "        model.add(layers.Dropout(0.25))\n",
    "        model.add(layers.Dense(4, activation=\"softmax\"))\n",
    "\n",
    "        for layer in baseModel.layers[:]:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        model.compile(optimizer=optimizers.Adam(lr = 0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        direc_path = '/home/ml/Fang/preprocess_vgg16/fbresult/'\n",
    "\n",
    "        \n",
    "        filepath=direc_path +\"weights.best.hdf5\"\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "        callbacks_list = [checkpoint]\n",
    "        \n",
    "        # Short training ONLY top layers \n",
    "        #... so the conv_base weights will not be destroyed by the random intialization of the new weights\n",
    "        history = model.fit(x_train, y_train, epochs=25, batch_size=32, validation_data=(x_val,y_val),shuffle=True,callbacks=callbacks_list, verbose=1)\n",
    "        #load the best top model\n",
    "        model.load_weights(filepath)\n",
    "        #model.compile(optimizer=optimizers.Adam(lr = 0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Make last block of the conv_base trainable:\n",
    "        for layer in baseModel.layers[:11]:\n",
    "            layer.trainable = False\n",
    "        for layer in baseModel.layers[11:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "        \n",
    "        # Compile frozen conv_base + UNfrozen top block + my top layer ... slower learning rate\n",
    "        model.compile(optimizer=optimizers.Adam(lr = 0.0001),\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        #train the unfrozen model\n",
    "        history = model.fit(x_train, y_train, epochs=25, batch_size=32, validation_data=(x_val,y_val),shuffle=True,callbacks=callbacks_list,verbose=1)\n",
    "\n",
    "        \n",
    "        model.load_weights(filepath)\n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(lr = 0.0001), loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "         \n",
    "        x_test = np.concatenate((test1x,test2x,test3x,test4x))              \n",
    "        y_pred = model.predict(x_test)\n",
    "\n",
    "        ypred = np.argmax(y_pred, axis=1)\n",
    "        ytest = np.argmax(y_test, axis=1)\n",
    "\n",
    "        cm = confusion_matrix(ytest, ypred)\n",
    "        cm_flatten = cm.flatten()\n",
    "        ps_cm = np.insert(cm_flatten,0,patch_size)\n",
    "        ps_cm = ps_cm.reshape(1,17)\n",
    "        \n",
    "        del (baseModel, checkpoint,callbacks_list)\n",
    "\n",
    "        test_accuracy = (np.trace(cm))/len(ytest)\n",
    "        size_update=patch_size\n",
    "        \n",
    "        print('RESULT: '+repr(size_update)+', '+repr(test_accuracy)+'\\n')\n",
    "        with open(direc_path+\"acc_height_back_train_fore_test.csv\", \"a\") as myfile:\n",
    "            myfile.write(repr(size_update)+', '+repr(test_accuracy)+'\\n')    \n",
    "        \n",
    "        p1predict = model.predict(test1x)\n",
    "        p2predict = model.predict(test2x)\n",
    "        p3predict = model.predict(test3x)\n",
    "        p4predict = model.predict(test4x)\n",
    "\n",
    "        del (test1x, test2x, test3x, test4x)\n",
    "        \n",
    "        p1_predict = np.concatenate([test_idx1,p1predict], axis=1)\n",
    "        p2_predict = np.concatenate([test_idx2,p2predict], axis=1)\n",
    "        p3_predict = np.concatenate([test_idx3,p3predict], axis=1)\n",
    "        p4_predict = np.concatenate([test_idx4,p4predict], axis=1)\n",
    "\n",
    "        with open(direc_path+'heapmap_height_back_train_fore_test_p1_ps100.csv','a') as f:\n",
    "            np.savetxt(f, p1_predict, fmt='%s')\n",
    "        with open(direc_path+'heapmap_height_back_train_fore_test_p2_ps100.csv','a') as f:\n",
    "            np.savetxt(f, p2_predict, fmt='%s')\n",
    "        with open(direc_path+'heapmap_height_back_train_fore_test_p3_ps100.csv','a') as f:\n",
    "            np.savetxt(f, p3_predict, fmt='%s')\n",
    "        with open(direc_path+'heapmap_height_back_train_fore_test_p4_ps100.csv','a') as f:\n",
    "            np.savetxt(f, p4_predict, fmt='%s')\n",
    "        with open(direc_path+'cm_height_back_train_fore_testt.csv','a') as f:\n",
    "            np.savetxt(f, ps_cm, fmt='%s')\n",
    "            \n",
    "       # with open(\"accuracy_resnet_ps200_9010.csv\", \"a\") as myfile:\n",
    "       #     myfile.write(repr(patch_size)+','+repr(test_accuracy)+'\\n')\n",
    "        \n",
    "        del (p1_predict,p2_predict,p3_predict,p4_predict, p1predict, p2predict, p3predict, p4predict)\n",
    "        \n",
    "        del ( x_train,x_val,y_train,y_val,x_test, y_test, model, y_pred, history )\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
