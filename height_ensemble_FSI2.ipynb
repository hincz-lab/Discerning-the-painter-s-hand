{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.feature_extraction.image import extract_patches\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Flatten, AveragePooling2D, Dropout, Input\n",
    "from keras import regularizers, optimizers, models, layers, losses, metrics\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report\n",
    "from skimage import img_as_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load data without any changes (high resolution)\n",
    "def load_data(file_path):\n",
    "    data = cv2.imread(file_path,cv2.IMREAD_UNCHANGED)\n",
    "    data = img_as_float(data)*255\n",
    "    return data\n",
    "\n",
    "# divide each channel of the painting to patches then concatenate channels, also get the list of\n",
    "# corresponding labels (painter id)\n",
    "def get_patches(data, patch_size, painter_id):\n",
    "    pc1 = extract_patches(data[:,:,0], patch_shape = patch_size, extraction_step = patch_size)\n",
    "    pc1 = pc1.reshape(-1, patch_size, patch_size)\n",
    "    pc2 = extract_patches(data[:,:,1], patch_shape = patch_size, extraction_step = patch_size)\n",
    "    pc2 = pc2.reshape(-1, patch_size, patch_size)\n",
    "    pc3 = extract_patches(data[:,:,2], patch_shape = patch_size, extraction_step = patch_size)\n",
    "    pc3 = pc3.reshape(-1, patch_size, patch_size)\n",
    "    pc1_reshaped = pc1.reshape(*pc1.shape,1)\n",
    "    pc2_reshaped = pc2.reshape(*pc2.shape,1)\n",
    "    pc3_reshaped = pc3.reshape(*pc3.shape,1)\n",
    "    patches = np.concatenate((pc1_reshaped,pc2_reshaped,pc3_reshaped),axis=3)\n",
    "    \n",
    "    labels = []\n",
    "    def get_label(painter_id, patch_len):\n",
    "        labels.clear()\n",
    "        labels.append(painter_id * patch_len)\n",
    "        return labels\n",
    "\n",
    "    list_len = np.ones(len(patches))\n",
    "    y_list = get_label(painter_id, list_len)\n",
    "    y_list = np.reshape(y_list,(len(patches),1)) \n",
    "                        \n",
    "    return patches, y_list  # use this when shuffle=False\n",
    "\n",
    "\n",
    "# preprocess each patches to prepare for transfer learning by subtracting the mean [103.939, 116.779, 123.68]\n",
    "def preprocess_patches(patch_list):\n",
    "    patches = preprocess_input(patch_list)\n",
    "    return patches\n",
    "\n",
    "# resize patches to 224*224\n",
    "def resize_patches(patch_list):   \n",
    "    resize_patches = [None]*len(patch_list)\n",
    "    for i in range(len(patch_list)):\n",
    "        resize_patches[i] = cv2.resize(patch_list[i],(224, 224))\n",
    "    new_list = np.asarray(resize_patches, dtype=np.float64)\n",
    "    return new_list\n",
    "    \n",
    "\n",
    "def process_pipeline(file_path, patch_size, painter_id):\n",
    "    data = load_data(file_path)\n",
    "    patch_list, labels = get_patches(data, patch_size, painter_id)\n",
    "    preprocessed_patches = preprocess_patches(patch_list)\n",
    "    resized_patches = resize_patches(preprocessed_patches)\n",
    "    return resized_patches, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "psizes = [300]\n",
    "\n",
    "for patch_size in psizes:\n",
    "    print('PATCH SIZE: '+repr(patch_size))\n",
    "    \n",
    "    p1a_x, p1a_y = process_pipeline('/home/ml/FINAL_CODE_DATA/height_data/fgp1.png', patch_size, 0)\n",
    "    p1b_x, p1b_y = process_pipeline('/home/ml/FINAL_CODE_DATA/height_data/fgp2.png', patch_size, 0)\n",
    "    p1c_x, p1c_y = process_pipeline('/home/ml/FINAL_CODE_DATA/height_data/fgp3.png', patch_size, 0)\n",
    "    p2a_x, p2a_y = process_pipeline('/home/ml/FINAL_CODE_DATA/height_data/fgp4.png', patch_size, 1)\n",
    "    p2b_x, p2b_y = process_pipeline('/home/ml/FINAL_CODE_DATA/height_data/fgp5.png', patch_size, 1)\n",
    "    p2c_x, p2c_y = process_pipeline('/home/ml/FINAL_CODE_DATA/height_data/fgp6.png', patch_size, 1)\n",
    "    p3a_x, p3a_y = process_pipeline('/home/ml/FINAL_CODE_DATA/height_data/fgp7.png', patch_size, 2)\n",
    "    p3b_x, p3b_y = process_pipeline('/home/ml/FINAL_CODE_DATA/height_data/fgp8.png', patch_size, 2)\n",
    "    p3c_x, p3c_y = process_pipeline('/home/ml/FINAL_CODE_DATA/height_data/fgp9.png', patch_size, 2)\n",
    "    p4a_x, p4a_y = process_pipeline('/home/ml/FINAL_CODE_DATA/height_data/fgp10.png', patch_size, 3)\n",
    "    p4c_x, p4c_y = process_pipeline('/home/ml/FINAL_CODE_DATA/height_data/fgp12.png', patch_size, 3)\n",
    "\n",
    "    p4_b = cv2.imread('/home/ml/FINAL_CODE_DATA/height_data/fgp11.png',cv2.IMREAD_UNCHANGED)  \n",
    "    p4_b = img_as_float(p4_b)*255\n",
    "    p4_b = cv2.rotate(p4_b, cv2.ROTATE_180)\n",
    "    p4b_x, p4b_y = get_patches(p4_b, patch_size, 3)\n",
    "    p4b_x = resize_patches(preprocess_patches(p4b_x))\n",
    "    del p4_b\n",
    "\n",
    "    x_train_val = np.concatenate((p1a_x, p1c_x, \n",
    "                                  p2a_x, p2c_x, \n",
    "                                  p3a_x, p3c_x, \n",
    "                                  p4a_x, p4c_x))\n",
    "    y_train_val = np.concatenate((p1a_y, p1c_y, \n",
    "                                  p2a_y, p2c_y, \n",
    "                                  p3a_y, p3c_y, \n",
    "                                  p4a_y, p4c_y))\n",
    "    del(p1a_x,p1a_y,p2a_x,p2a_y,p3a_x,p3a_y,p4a_x,p4a_y)\n",
    "    del(p1c_x,p1c_y,p2c_x,p2c_y,p3c_x,p3c_y,p4c_x,p4c_y)\n",
    "    \n",
    "    foldnum=20\n",
    "    \n",
    "    for fold in range(0, foldnum):\n",
    "    \n",
    "        ##A&C as training+validation data\n",
    "        \n",
    "        \n",
    "        x_train,x_val,y_train,y_val = train_test_split(x_train_val, y_train_val, test_size=0.1)\n",
    "        \n",
    "        x_test = np.concatenate((p1b_x, p2b_x, p3b_x, p4b_x))\n",
    "        \n",
    "        # one-hot encode y\n",
    "        y_train = to_categorical(y_train, num_classes=None)\n",
    "        y_val = to_categorical(y_val, num_classes=None)\n",
    "        \n",
    "        y_test = np.concatenate((p1b_y, p2b_y, p3b_y, p4b_y))\n",
    "        y_test = to_categorical(y_test, num_classes=None)\n",
    "\n",
    "        test_idx = np.arange(len(p1b_x)).reshape(len(p1b_x),1)\n",
    "        \n",
    "        \n",
    "        #################\n",
    "        #if run into \"value error\", copy the rest of the code to a new cell and rerun it\n",
    "        # \"value error\" happens when the frozen network performs better than the network with layers unlocked\n",
    "        #################\n",
    "        \n",
    "        baseModel = VGG16(weights=\"imagenet\", include_top=False,input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "        model = models.Sequential()\n",
    "        model.add(baseModel)\n",
    "        model.add(layers.AveragePooling2D(pool_size=(3, 3)))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dropout(0.25))\n",
    "        model.add(layers.Dense(64, activation='relu',kernel_regularizer=regularizers.l2(0.001)))\n",
    "        model.add(layers.Dropout(0.25))\n",
    "        model.add(layers.Dense(4, activation=\"softmax\"))\n",
    "        \n",
    "        try:\n",
    "            for layer in baseModel.layers[:]:\n",
    "                layer.trainable = False\n",
    "\n",
    "        \n",
    "            model.compile(optimizer=optimizers.Adam(lr = 0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            filepath=\"weights.best.hdf5\"\n",
    "            checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "            callbacks_list = [checkpoint]\n",
    "        \n",
    "            # training only top layers \n",
    "            history = model.fit(x_train, y_train, epochs=25, batch_size=32, validation_data=(x_val,y_val),shuffle=True,callbacks=callbacks_list, verbose=2)\n",
    "            #load the best top model\n",
    "            model.load_weights(\"weights.best.hdf5\")\n",
    "            #model.compile(optimizer=optimizers.Adam(lr = 0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "            # Make last block of the conv_base trainable:\n",
    "            for layer in baseModel.layers[:11]:\n",
    "                layer.trainable = False\n",
    "            for layer in baseModel.layers[11:]:\n",
    "                layer.trainable = True\n",
    "\n",
    "        \n",
    "            # Compile frozen conv_base + UNfrozen top block + my top layer, use a slower learning rate\n",
    "            model.compile(optimizer=optimizers.Adam(lr = 0.0001),\n",
    "                          loss='categorical_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "            #train the unfrozen model\n",
    "            history = model.fit(x_train, y_train, epochs=25, batch_size=32, validation_data=(x_val,y_val),shuffle=True,callbacks=callbacks_list,verbose=2)\n",
    "\n",
    "        \n",
    "            model.load_weights(\"weights.best.hdf5\")\n",
    "\n",
    "            model.compile(optimizer=optimizers.Adam(lr = 0.0001), loss='categorical_crossentropy', \n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "            y_pred = model.predict(x_test)\n",
    "            ypred = np.argmax(y_pred, axis=1)\n",
    "            ytest = np.argmax(y_test, axis=1)\n",
    "\n",
    "            cm = confusion_matrix(ytest, ypred)\n",
    "            cm_flatten = cm.flatten()\n",
    "            ps_cm = np.insert(cm_flatten,0,patch_size)\n",
    "            ps_cm = ps_cm.reshape(1,17)\n",
    "\n",
    "            test_accuracy = (np.trace(cm))/len(ytest)\n",
    "            print('RESULT: '+repr(patch_size)+', '+repr(test_accuracy)+'\\n')\n",
    "        \n",
    "            p1predict = model.predict(p1b_x)\n",
    "            p2predict = model.predict(p2b_x)\n",
    "            p3predict = model.predict(p3b_x)\n",
    "            p4predict = model.predict(p4b_x)\n",
    "\n",
    "            p1_predict = np.concatenate([test_idx,p1predict], axis=1)\n",
    "            p2_predict = np.concatenate([test_idx,p2predict], axis=1)\n",
    "            p3_predict = np.concatenate([test_idx,p3predict], axis=1)\n",
    "            p4_predict = np.concatenate([test_idx,p4predict], axis=1)\n",
    "\n",
    "            with open('height_p1_ps300.csv','a') as f:\n",
    "                np.savetxt(f, p1_predict, fmt='%s')\n",
    "            with open('height_p2_ps300.csv','a') as f:\n",
    "                np.savetxt(f, p2_predict, fmt='%s')\n",
    "            with open('height_p3_ps300.csv','a') as f:\n",
    "                np.savetxt(f, p3_predict, fmt='%s')\n",
    "            with open('height_p4_ps300.csv','a') as f:\n",
    "                np.savetxt(f, p4_predict, fmt='%s')\n",
    "            with open('height_cm_ps300.csv','a') as f:\n",
    "                np.savetxt(f, ps_cm, fmt='%s')\n",
    "            \n",
    "            with open(\"height_accuracy_ps300.csv\", \"a\") as myfile:\n",
    "                myfile.write(repr(patch_size)+','+repr(test_accuracy)+'\\n')\n",
    "            \n",
    "            report = classification_report(ytest, ypred,output_dict=True)\n",
    "            p1_report = np.asarray([report['0']['f1-score'] ])\n",
    "            p1_report = np.insert(p1_report,0,patch_size)\n",
    "            p1_report = p1_report.reshape(1,2)\n",
    "\n",
    "            p2_report = np.asarray([report['1']['f1-score'] ])\n",
    "            p2_report = np.insert(p2_report,0,patch_size)\n",
    "            p2_report = p2_report.reshape(1,2)\n",
    "\n",
    "            p3_report = np.asarray([report['2']['f1-score'] ])\n",
    "            p3_report = np.insert(p3_report,0,patch_size)\n",
    "            p3_report = p3_report.reshape(1,2)\n",
    "\n",
    "            p4_report = np.asarray([report['3']['f1-score'] ])\n",
    "            p4_report = np.insert(p4_report,0,patch_size)\n",
    "            p4_report = p4_report.reshape(1,2)\n",
    "\n",
    "            overall = np.asarray([report['accuracy']])\n",
    "            overall = np.insert(overall,0,patch_size)\n",
    "            overall = overall.reshape(1,2)\n",
    "        \n",
    "\n",
    "            with open('height_p1_f1_ps300.csv','a') as f:\n",
    "                np.savetxt(f, p1_report, fmt='%s')\n",
    "            with open('height_p2_f1_ps300.csv','a') as f:\n",
    "                np.savetxt(f, p2_report, fmt='%s')\n",
    "            with open('height_p3_f1_ps300.csv','a') as f:\n",
    "                np.savetxt(f, p3_report, fmt='%s')\n",
    "            with open('height_p4_f1_ps300.csv','a') as f:\n",
    "                np.savetxt(f, p4_report, fmt='%s')\n",
    "            with open('height_overall_ps300.csv','a') as f:\n",
    "                np.savetxt(f, overall, fmt='%s')\n",
    "        \n",
    "            \n",
    "            del(x_train,x_val,x_test,y_train,y_val,y_test,test_idx,p1predict,p2predict,p3predict,p4predict,p1_predict,p2_predict,p3_predict,p4_predict,model,ps_cm,cm,history)\n",
    "            del(y_pred,ypred,ytest,test_accuracy,filepath)\n",
    "            del(p1_report,p2_report,p3_report,p4_report,overall)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseModel = VGG16(weights=\"imagenet\", include_top=False,input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(baseModel)\n",
    "model.add(layers.AveragePooling2D(pool_size=(3, 3)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.Dense(64, activation='relu',kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.Dense(4, activation=\"softmax\"))\n",
    "\n",
    "for layer in baseModel.layers[:]:\n",
    "    layer.trainable = False\n",
    "\n",
    "        \n",
    "model.compile(optimizer=optimizers.Adam(lr = 0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "filepath=\"weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "        \n",
    "# Short training ONLY top layers \n",
    "#... so the conv_base weights will not be destroyed by the random intialization of the new weights\n",
    "history = model.fit(x_train, y_train, epochs=25, batch_size=32, validation_data=(x_val,y_val),shuffle=True,callbacks=callbacks_list, verbose=2)\n",
    "#load the best top model\n",
    "model.load_weights(\"weights.best.hdf5\")\n",
    "#model.compile(optimizer=optimizers.Adam(lr = 0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "# Make last block of the conv_base trainable:\n",
    "for layer in baseModel.layers[:11]:\n",
    "    layer.trainable = False\n",
    "for layer in baseModel.layers[11:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "        \n",
    "# Compile frozen conv_base + UNfrozen top block + my top layer ... slower learning rate\n",
    "model.compile(optimizer=optimizers.Adam(lr = 0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#train the unfrozen model\n",
    "history = model.fit(x_train, y_train, epochs=25, batch_size=32, validation_data=(x_val,y_val),shuffle=True,callbacks=callbacks_list,verbose=2)\n",
    "\n",
    "        \n",
    "model.load_weights(\"weights.best.hdf5\")\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr = 0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "y_pred = model.predict(x_test)\n",
    "ypred = np.argmax(y_pred, axis=1)\n",
    "ytest = np.argmax(y_test, axis=1)\n",
    "\n",
    "cm = confusion_matrix(ytest, ypred)\n",
    "\n",
    "test_accuracy = (np.trace(cm))/len(ytest)\n",
    "print('RESULT: '+repr(patch_size)+', '+repr(test_accuracy)+'\\n')\n",
    "        \n",
    "p1predict = model.predict(p1b_x)\n",
    "p2predict = model.predict(p2b_x)\n",
    "p3predict = model.predict(p3b_x)\n",
    "p4predict = model.predict(p4b_x)\n",
    "\n",
    "p1_predict = np.concatenate([test_idx,p1predict], axis=1)\n",
    "p2_predict = np.concatenate([test_idx,p2predict], axis=1)\n",
    "p3_predict = np.concatenate([test_idx,p3predict], axis=1)\n",
    "p4_predict = np.concatenate([test_idx,p4predict], axis=1)\n",
    "\n",
    "with open('heapmap_p1_ps140_9010.csv','a') as f:\n",
    "            np.savetxt(f, p1_predict, fmt='%s')\n",
    "with open('heapmap_p2_ps140_9010.csv','a') as f:\n",
    "            np.savetxt(f, p2_predict, fmt='%s')\n",
    "with open('heapmap_p3_ps140_9010.csv','a') as f:\n",
    "            np.savetxt(f, p3_predict, fmt='%s')\n",
    "with open('heapmap_p4_ps140_9010.csv','a') as f:\n",
    "            np.savetxt(f, p4_predict, fmt='%s')\n",
    "with open('cm_height_vgg16_ps140_9010.csv','a') as f:\n",
    "            np.savetxt(f, ps_cm, fmt='%s')\n",
    "            \n",
    "with open(\"accuracy_vgg16_ps140_9010.csv\", \"a\") as myfile:\n",
    "            myfile.write(repr(patch_size)+','+repr(test_accuracy)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.loadtxt('heapmap_p1_200_8020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(np.arange(0,3), 3, replace=False)[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate((np.random.choice(np.arange(0,3), 3, replace=False)[0:2],\n",
    "                np.random.choice(np.arange(3,6), 3, replace=False)[0:2],\n",
    "                np.random.choice(np.arange(6,9), 3, replace=False)[0:2],\n",
    "                np.random.choice(np.arange(9,12), 3, replace=False)[0:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pick_test = np.asarray([np.random.choice(np.arange(0,3), 3, replace=False)[0],\n",
    "                                    np.random.choice(np.arange(3,6), 3, replace=False)[0],\n",
    "                                    np.random.choice(np.arange(6,9), 3, replace=False)[0],\n",
    "                                    np.random.choice(np.arange(9,12), 3, replace=False)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pick_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray([np.random.choice(np.arange(0,3), 3, replace=False)[0],\n",
    "                 np.random.choice(np.arange(3,6), 3, replace=False)[0],\n",
    "                 np.random.choice(np.arange(6,9), 3, replace=False)[0],\n",
    "                 np.random.choice(np.arange(9,12), 3, replace=False)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
